# -*- coding: utf-8 -*-
"""SVM_LogisticRegression_SGD_HalfSpaceClassifier_library.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10rE6NZfrNpdJ19zgErcnxCnEZo8YtZti
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier

#Reading the dataset

df=pd.read_csv("ML3-DATASET.csv")
df.head(10)

# Dataframe for target values 0 and 1

df0=df[df['class']==0]
df1=df[df['class']==1]

#Plotting the curve against two features

plt.scatter(df0['variance'],df0["skewness"],color="red", marker="+")
plt.scatter(df1['variance'],df1["skewness"],color="blue", marker=".")

# Using all featurs except the target for training the model
X=df.drop(['class'],axis='columns')
X.head()
y=df['class']

#Slitting the dataset into training and testing in 70-30 or 80-20 or 90-10
x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3)

#Implenettaion of Support Vector machine using sklearn

from sklearn.svm import SVC

#Svm Classifier using Linear kernel
model=SVC(kernel="linear")

#Using object of svm class and traing model
model.fit(x_train,y_train)
model.predict(x_test)
print(model.score(x_test,y_test))

# get number of support vectors for each class
print(model.n_support_)

#Getting parameter coefficients and intercept
print(model.coef_,model.intercept_)

#Svm Classifier using Polynomial kernel using different regularization parameter
model1=SVC(kernel="poly",C=100)

model1.fit(x_train,y_train)
model1.predict(x_test)
print(model1.score(x_test,y_test))
print(model1.n_support_)

#Svm Classifier using Gaussian kernel using different regularization parameter
model0=SVC()

model0.fit(x_train,y_train)
model0.predict(x_test)
print(model0.score(x_test,y_test))
print(model0.n_support_)
#Logistic Regression from sklearn

model2=LogisticRegression()
model2.fit(x_train,y_train)
model2.predict(x_test)
print(model2.score(x_test,y_test))

#Logistic Regression using Stochastic Gradient Descent
# Increased the maximum iteration for the convergence of curve

model3=LogisticRegression(solver='sag',max_iter=200 , C=0.1)
model3.fit(x_train,y_train)
model3.predict(x_test)
print(model3.score(x_test,y_test))


#Stochastic Gradient Descent procedure
sgd_classifier=SGDClassifier()
sgd_classifier.fit(x_train,y_train)
l=x_test
#print(l)
#print(sgd_classifier.predict(l))

#Half Space Classifier Using Perceptron

from sklearn.linear_model import Perceptron
clf= Perceptron(random_state=0)
clf.fit(x_train, y_train)
print(clf.score(X, y))