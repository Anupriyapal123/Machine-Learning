# -*- coding: utf-8 -*-
"""K_mean_Clustering_Self.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Qnd3B0qbawqif5K-vun31MDaYhDH0up
"""

#%tensorflow_version 2.x
!pip install mnist
#import tensorflow
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import mnist


np.random.seed(4)

# Calculating euclidean distance between two feature vectors.
def euc_dist(x1,x2):
  return np.sqrt(np.sum(x1-x2)**2)

# class for performing k-means clustering
class kmeans:

 # initialising the values
  def __init__(self, K=10, max_iters=10, plot_steps=False):
    self.K=K
    self.max_iters=max_iters
    self.plot_steps=plot_steps

    self.clusters=[[] for _ in range(self.K)]
    self.centroids=[]
 
 # predict using the sample examples
  def predict(self,X):
    self.X=X
    self.n_samples,self.n_features=X.shape
    print(X.shape)
    print(self.n_samples,784)

    #randomly initialize the n centroids
    random_centroid_idx=np.random.choice(self.n_samples , self.K , replace=False)
    self.centroids=[self.X[idx] for idx in random_centroid_idx]
    for _ in range(self.max_iters):
        self.clusters=self.create_clusters(self.centroids)
        centroid_old=self.centroids
        self.centroids=self.get_centroid(self.clusters)

        #checking whether distance between new centroid and old centroid is zero
        if self.converged(centroid_old,self.centroids):
          break
    return self.cluster_labels(self.clusters)               

    #forming list of samples belong each cluster  
  def cluster_labels(self,clusters):
    labels=np.empty(self.n_samples)

    for cluster_idx,cluster in enumerate (clusters):
      for sample_idx in cluster:
        labels[sample_idx]=cluster_idx
    return labels



# appending the examples to the nearest centroid for each cluster and creating the cluster.

  def create_clusters(self, centroids):
      clusters=[[] for _ in range(self.K)]
      for idx,sample in enumerate(self.X):
        centroid_idx=self.nearest_centroid(sample,centroids)
        clusters[centroid_idx].append(idx)

      return clusters

  # calculating the closest centroid
  def nearest_centroid(self,sample,centroids):
      distances=[euc_dist(sample,point) for point in centroids]

      closest_idx=np.argmin(distances)
      return closest_idx

#Updating the centroid in each iteration

  def get_centroid(self,clusters):
    centroids=np.zeros((self.K,784))
    for cluster_idx,cluster in enumerate(clusters):
      cluster_mean=np.mean(self.X[cluster],axis=0)
      centroids[cluster_idx]=cluster_mean
    return centroids

# to check the convergence in each iteration.
  def converged(self,centroid_old,centroids):

     distances=[euc_dist(centroid_old[i], centroids[i]) for i in range (self.K)]
     print(distances)
     return sum(distances)==0
  
  

# loading the mnist dataset
train_sample=mnist.train_images()
train_labels=mnist.train_labels()

#normalising the dataset
train_sample=train_sample/255.0

#Flattening the dataset
train_sample=train_sample.reshape((-1,784))

train_sample=train_sample.reshape((-1,784))
print(train_sample.shape)

# The number of clusters will be number of unique labels 
cluster=len(np.unique(train_labels))
print(cluster)

k=kmeans(K=cluster, max_iters=10,plot_steps=False)
y=k.predict(train_sample)